{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from glob import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "repo_path = \"..\"\n",
    "import sys\n",
    "sys.path.append(f\"{repo_path}/code/\")\n",
    "\n",
    "font = {'size'   : 14}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "figure = {'figsize'   : (12,8),\n",
    "          'max_open_warning': False}\n",
    "matplotlib.rc('figure', **figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "inf_history_path = (\"../data/models/b_2/\"\n",
    "                    \"ne_200_lr_1e-06_bs_1000_t_0.1/\"\n",
    "                    \"init_[0-9]/history.json\")\n",
    "\n",
    "\n",
    "for h_path in glob(inf_history_path):\n",
    "  with open(h_path) as h_file:\n",
    "    h_dict  = json.load(h_file)\n",
    "  \n",
    "  valid_step, valid_loss = np.array(h_dict[\"loss_valid\"]).T\n",
    "\n",
    "  ax.plot(valid_loss)\n",
    "  \n",
    "ax.set_xlabel(\"training epoch\")\n",
    "ax.set_ylabel(\"validation-set inference-aware loss\")\n",
    "\n",
    "fig.savefig(\"../paper/gfx/figure4a.pdf\",bbox_inches='tight')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from template_model import TemplateModel\n",
    "from glob import glob\n",
    "\n",
    "from importlib import reload\n",
    "import template_model\n",
    "reload(template_model)\n",
    "\n",
    "tm = TemplateModel(multiple_pars=True)\n",
    "\n",
    "inf_template_path = \"../data/models/b_2/ne_200_lr_1e-06_bs_1000_t_0.1/init_[0-9]/templates.json\"\n",
    "clf_template_path = \"../data/models/cross_entropy/ne_200_lr_0.001_bs_32/init_[0-9]/templates.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "step_size = 0.1\n",
    "n_steps = 100\n",
    "d = 0.0\n",
    "pars = [\"r_dist\",\"b_rate\"]\n",
    "\n",
    "s_exp_scan = np.linspace(20.,80.,61, endpoint=True)\n",
    "par_phs = {tm.r_dist : 2.0*np.ones_like(s_exp_scan),\n",
    "           tm.b_rate : 3.0*np.ones_like(s_exp_scan),\n",
    "           tm.s_exp :  s_exp_scan,\n",
    "           tm.b_exp : 1000.0*np.ones_like(s_exp_scan)} \n",
    "\n",
    "\n",
    "inf_pls = {}\n",
    "for inf_path in glob(inf_template_path):\n",
    "  with tf.Session() as sess:\n",
    "    tm.templates_from_json(inf_path)\n",
    "    # get asimov data (default pars)\n",
    "    asimov_data = tm.asimov_data(sess=sess)\n",
    "\n",
    "    obs_phs = {tm.obs : asimov_data}\n",
    "    mod_phs = par_phs.copy()\n",
    "    # get likelihood before changing pars\n",
    "    nll, sub_hess, sub_grad = tm.hessian_and_gradient(pars=pars,\n",
    "                                                      par_phs=mod_phs, obs_phs=obs_phs)\n",
    "    # profile likelihood with Newton method\n",
    "    for i in range(n_steps):\n",
    "      newton_step =  np.matmul(np.linalg.inv(sub_hess+d*np.ones([len(pars)])),sub_grad[:,:,np.newaxis])\n",
    "      mod_phs[tm.r_dist] = mod_phs[tm.r_dist] - step_size*newton_step[:,0,0]\n",
    "      mod_phs[tm.b_rate] = mod_phs[tm.b_rate] - step_size*newton_step[:,1,0]\n",
    "      p_nll, sub_hess, sub_grad = tm.hessian_and_gradient(pars=[\"r_dist\",\"b_rate\"],\n",
    "                                                        par_phs=mod_phs, obs_phs=obs_phs)\n",
    "    \n",
    "    inf_pls[inf_path] = p_nll\n",
    "    print(\"nll - p_nll\",(p_nll-nll).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 0.1\n",
    "n_steps = 100\n",
    "d = 0.0\n",
    "pars = [\"r_dist\",\"b_rate\"]\n",
    "\n",
    "\n",
    "s_exp_scan = np.linspace(20.,80.,61, endpoint=True)\n",
    "par_phs = {tm.r_dist : 2.0*np.ones_like(s_exp_scan),\n",
    "           tm.b_rate : 3.0*np.ones_like(s_exp_scan),\n",
    "           tm.s_exp :  s_exp_scan,\n",
    "           tm.b_exp : 1000.0*np.ones_like(s_exp_scan)} \n",
    "\n",
    "\n",
    "clf_pls = {}\n",
    "for clf_path in glob(clf_template_path):\n",
    "  with tf.Session() as sess:\n",
    "    templates = tm.templates_from_json(clf_path)\n",
    "    # get asimov data (default pars)\n",
    "    asimov_data = tm.asimov_data(sess=sess)\n",
    "    obs_phs = {tm.obs : asimov_data}\n",
    "    mod_phs = par_phs.copy()\n",
    "    # get likelihood before changing pars\n",
    "    nll, sub_hess, sub_grad = tm.hessian_and_gradient(pars=[\"r_dist\",\"b_rate\"],\n",
    "                                                      par_phs=mod_phs, obs_phs=obs_phs)\n",
    "    # profile likelihood with Newton method\n",
    "    for i in range(n_steps):\n",
    "      newton_step =  np.matmul(np.linalg.inv(sub_hess+d*np.eye(len(pars))),sub_grad[:,:,np.newaxis])\n",
    "      mod_phs[tm.r_dist] = mod_phs[tm.r_dist] - step_size*newton_step[:,0,0]\n",
    "      mod_phs[tm.b_rate] = mod_phs[tm.b_rate] - step_size*newton_step[:,1,0]\n",
    "      p_nll, sub_hess, sub_grad = tm.hessian_and_gradient(pars=[\"r_dist\",\"b_rate\"],\n",
    "                                                        par_phs=mod_phs, obs_phs=obs_phs)\n",
    "\n",
    "    \n",
    "    clf_pls[clf_path] = p_nll\n",
    "    print(\"nll - p_nll\",(p_nll-nll).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import InterpolatedUnivariateSpline\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "inf_roots = {}\n",
    "for name, p_nll in inf_pls.items():\n",
    "  shift_nll = p_nll-p_nll.min()\n",
    "  inf_roots[name] = InterpolatedUnivariateSpline(s_exp_scan, shift_nll-0.5).roots()\n",
    "  inf_line = ax.plot(s_exp_scan, shift_nll ,\"b\",alpha=0.3)\n",
    "\n",
    "clf_roots = {}\n",
    "for name, p_nll in clf_pls.items():\n",
    "  shift_nll = p_nll-p_nll.min()\n",
    "  clf_roots[name] = InterpolatedUnivariateSpline(s_exp_scan, shift_nll-0.5).roots()\n",
    "  clf_line = ax.plot(s_exp_scan, shift_nll,\"r\", alpha=0.3)\n",
    "\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.set_xlim(left=s_exp_scan.min(), right=s_exp_scan.max())\n",
    "\n",
    "ax.set_xlabel(\"$s$ parameter of interest\")\n",
    "ax.set_ylabel(r\"profiled likelihood $\\Delta(\\mathcal{-\\ln L})$\")\n",
    "\n",
    "\n",
    "inf_mean = np.mean([(r[1]-r[0])/2. for r in inf_roots.values()])\n",
    "inf_std = np.std([(r[1]-r[0])/2. for r in inf_roots.values()],ddof=1)\n",
    "clf_mean = np.mean([(r[1]-r[0])/2. for r in clf_roots.values()])\n",
    "clf_std = np.std([(r[1]-r[0])/2. for r in clf_roots.values()],ddof=1)\n",
    "print(f\"INFERNO widths {inf_mean} pm {inf_std}\")\n",
    "print(f\"NN Classifier widths {clf_mean} pm {clf_std}\")\n",
    "\n",
    "\n",
    "ax.legend((clf_line[0], inf_line[0]), (\"cross-entropy\",\"inference-aware\"),\n",
    "          loc=\"upper center\",frameon=False)\n",
    "\n",
    "\n",
    "fig.savefig(\"../paper/gfx/figure4b.pdf\",bbox_inches='tight')\n",
    "\n",
    "\n",
    "fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
